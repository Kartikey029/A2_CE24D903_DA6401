{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6d88654",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import Adam\n",
    "import torch.optim as optim  # for NAdam\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "# -------------------------------\n",
    "# Helper: Activation Function Getter\n",
    "# -------------------------------\n",
    "def get_activation(activation):\n",
    "    \"\"\"\n",
    "    Returns an activation function based on the given input.\n",
    "    Accepts a string (case insensitive) or a callable.\n",
    "\n",
    "    Available options:\n",
    "      \"none\" (for no activation),\n",
    "      \"relu\", \"tanh\", \"sigmoid\", \"leaky_relu\", \"gelu\", \"silu\", \"mish\", \"selu\".\n",
    "    For \"mish\": mish(x) = x * tanh(softplus(x))\n",
    "    \"\"\"\n",
    "    if isinstance(activation, str):\n",
    "        act = activation.lower()\n",
    "        if act == \"none\":\n",
    "            return lambda x: x  # Identity function (no activation)\n",
    "        activation_dict = {\n",
    "            'relu': F.relu,\n",
    "            'tanh': torch.tanh,\n",
    "            'sigmoid': torch.sigmoid,\n",
    "            'leaky_relu': F.leaky_relu,\n",
    "            'gelu': F.gelu,\n",
    "            'silu': F.silu,\n",
    "            'mish': lambda x: x * torch.tanh(F.softplus(x)),\n",
    "            'selu': F.selu\n",
    "        }\n",
    "        return activation_dict.get(act, F.relu)\n",
    "    return activation\n",
    "\n",
    "# -------------------------------\n",
    "# Model Definition: iNaturalistCNN\n",
    "# -------------------------------\n",
    "class iNaturalistCNN(pl.LightningModule):\n",
    "    def __init__(self, num_classes=10, learning_rate=0.001, input_size=128,\n",
    "                 conv_configs=[(32, 3), (64, 3), (128, 3), (256, 3), (512, 3)],\n",
    "                 dense_neurons=512,\n",
    "                 dropout_rate=0.5,\n",
    "                 conv_activation='relu',\n",
    "                 dense_activation='relu',\n",
    "                 use_batch_norm=False,\n",
    "                 optimizer_type=\"adam\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes (int): Number of target classes.\n",
    "            learning_rate (float): Learning rate.\n",
    "            input_size (int): Image size (e.g. 224).\n",
    "            conv_configs (list of tuples): List of 5 tuples, each (num_filters, kernel_size).\n",
    "            dense_neurons (int): Number of neurons in the dense layer.\n",
    "            dropout_rate (float): Dropout probability.\n",
    "            conv_activation (str/callable): Activation for conv layers.\n",
    "            dense_activation (str/callable): Activation for the dense layer.\n",
    "            use_batch_norm (bool): Whether to include batch normalization.\n",
    "            optimizer_type (str): Type of optimizer: \"adam\" or \"nadam\".\n",
    "        \"\"\"\n",
    "        super(iNaturalistCNN, self).__init__()\n",
    "        self.save_hyperparameters(ignore=['conv_activation', 'dense_activation'])\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "\n",
    "        # Set activation functions.\n",
    "        self.conv_activation = get_activation(conv_activation)\n",
    "        self.dense_activation = get_activation(dense_activation)\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Build the convolutional layers.\n",
    "        if len(conv_configs) != 5:\n",
    "            raise ValueError(\"conv_configs must be a list of exactly five tuples.\")\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        if self.use_batch_norm:\n",
    "            self.bn_layers = nn.ModuleList()\n",
    "        in_channels = 3  # for RGB images\n",
    "        for out_channels, k_size in conv_configs:\n",
    "            conv_layer = nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=k_size,\n",
    "                stride=1,\n",
    "                padding=k_size // 2  # maintain spatial dimensions\n",
    "            )\n",
    "            self.conv_layers.append(conv_layer)\n",
    "            if self.use_batch_norm:\n",
    "                self.bn_layers.append(nn.BatchNorm2d(out_channels))\n",
    "            in_channels = out_channels\n",
    "\n",
    "        # Common max-pooling and dropout for convolutional blocks.\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout_conv = nn.Dropout2d(p=dropout_rate)\n",
    "\n",
    "        # Compute flattened feature dimension using a dummy input.\n",
    "        dummy_input = torch.zeros(1, 3, input_size, input_size)\n",
    "        x = dummy_input\n",
    "        for i, conv in enumerate(self.conv_layers):\n",
    "            x = conv(x)\n",
    "            if self.use_batch_norm:\n",
    "                x = self.bn_layers[i](x)\n",
    "            x = self.conv_activation(x)\n",
    "            x = self.pool(x)\n",
    "            x = self.dropout_conv(x)\n",
    "        self.feature_dim = x.view(1, -1).size(1)\n",
    "\n",
    "        # Fully-connected (dense) layers.\n",
    "        self.fc1 = nn.Linear(self.feature_dim, dense_neurons)\n",
    "        self.dropout_fc = nn.Dropout(p=dropout_rate)\n",
    "        self.fc2 = nn.Linear(dense_neurons, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional blocks: conv -> (optional BN) -> activation -> pool -> dropout.\n",
    "        for i, conv in enumerate(self.conv_layers):\n",
    "            x = conv(x)\n",
    "            if self.use_batch_norm:\n",
    "                x = self.bn_layers[i](x)\n",
    "            x = self.conv_activation(x)\n",
    "            x = self.pool(x)\n",
    "            x = self.dropout_conv(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the features.\n",
    "        x = self.dense_activation(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)  # Raw logits (F.cross_entropy applies softmax internally)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True,on_epoch=True)\n",
    "        self.log(\"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"test_loss\", loss,on_epoch=True,on_step=False)\n",
    "        self.log(\"test_acc\", acc, on_step=False, on_epoch=True, )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Choose the optimizer based on the hyperparameter.\n",
    "        optimizer_type = self.hparams.optimizer_type.lower()\n",
    "        weight_decay = getattr(self.hparams, \"weight_decay\", 1e-4)\n",
    "        if optimizer_type == \"adam\":\n",
    "            optimizer = Adam(self.parameters(), lr=self.hparams.learning_rate,weight_decay = weight_decay)\n",
    "            \n",
    "        elif optimizer_type == \"nadam\":\n",
    "            optimizer = torch.optim.NAdam(self.parameters(), lr=self.hparams.learning_rate,weight_decay = weight_decay)\n",
    "             \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer type: {optimizer_type}\")\n",
    "        return optimizer\n",
    "\n",
    "# -------------------------------\n",
    "# Data Splitting with Stratified Sampling\n",
    "# -------------------------------\n",
    "def get_train_val_split(dataset, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Given an ImageFolder dataset, splits the indices into training (80%) and\n",
    "    validation (20%) sets using stratified sampling.\n",
    "    \"\"\"\n",
    "    targets = dataset.targets\n",
    "    indices = list(range(len(dataset)))\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    train_idx, val_idx = next(sss.split(indices, targets))\n",
    "    return train_idx, val_idx\n",
    "\n",
    "# -------------------------------\n",
    "# Data Transforms (with or without augmentation)\n",
    "# -------------------------------\n",
    "def get_transforms(use_data_aug):\n",
    "    if use_data_aug:\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.Resize((128,128)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(20),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    else:\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.Resize((128,128)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    # For test/validation, use minimal transforms.\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((128,128)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    return transform_train, transform_test\n",
    "\n",
    "# -------------------------------\n",
    "# Main Training Function (sweep-compatible)\n",
    "# -------------------------------\n",
    "def run_training():\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "\n",
    "    # Process filter organization and kernel size.\n",
    "    kernel_size = config.get(\"kernel_size\", 3)\n",
    "    filt_org = config.get(\"filter_organization\", \"constant\")\n",
    "    if filt_org == \"constant\":\n",
    "        conv_configs = [(config.constant_filter, kernel_size)] * 5\n",
    "    elif filt_org == \"doubling\":\n",
    "        base = config.base_filter\n",
    "        conv_configs = [(base * (2 ** i), kernel_size) for i in range(5)]\n",
    "    elif filt_org == \"halving\":\n",
    "        base = config.base_filter\n",
    "        conv_configs = [(base // (2 ** i), kernel_size) for i in range(5)]\n",
    "    else:\n",
    "        raise ValueError(\"Unknown filter organization!\")\n",
    "    \n",
    "    # Data augmentation setup.\n",
    "    use_data_aug = config.get(\"data_augmentation\", True)\n",
    "    transform_train, transform_test = get_transforms(use_data_aug)\n",
    "\n",
    "    # Load the full training dataset from train_data_path.\n",
    "    train_dataset_full = ImageFolder(root=config.train_data_path, transform=transform_train)\n",
    "    # Split the full training dataset into training (80%) and validation (20%).\n",
    "    train_idx, val_idx = get_train_val_split(train_dataset_full, test_size=0.2, random_state=42)\n",
    "    train_dataset = Subset(train_dataset_full, train_idx)\n",
    "    val_dataset = Subset(train_dataset_full, val_idx)\n",
    "\n",
    "    # Load the test dataset from test_data_path.\n",
    "    test_dataset = ImageFolder(root=config.test_data_path, transform=transform_test)\n",
    "\n",
    "    # Create data loaders.\n",
    "    train_loader = DataLoader(train_dataset, batch_size=int(config.batch_size), shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=int(config.batch_size), shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=int(config.batch_size), shuffle=False, num_workers=4)\n",
    "\n",
    "    # Create the model.\n",
    "    model = iNaturalistCNN(\n",
    "        num_classes=10,\n",
    "        learning_rate=config.learning_rate,\n",
    "        input_size=128,\n",
    "        conv_configs=conv_configs,\n",
    "        dense_neurons=int(config.dense_neurons),\n",
    "        dropout_rate=config.dropout_rate,\n",
    "        conv_activation=config.conv_activation,\n",
    "        dense_activation=config.dense_activation,\n",
    "        use_batch_norm=config.batch_norm,\n",
    "        optimizer_type=config.optimizer  # Use the optimizer hyperparameter here.\n",
    "    )\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # Set up the Wandb logger.\n",
    "    wandb_logger = WandbLogger(project=config.project_name)\n",
    "        \n",
    "    # Define early stopping\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",       # metric to watch\n",
    "        patience=3,               # number of epochs with no improvement after which training will be stopped\n",
    "        mode=\"min\",               # because we want to minimize val_loss\n",
    "        verbose=True\n",
    "    )\n",
    "    # Define the PyTorch Lightning trainer.\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=int(config.max_epochs),\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        logger=wandb_logger,\n",
    "    )\n",
    "    \n",
    "    # Train the model using training and validation data.\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    # Test the final model on the test dataset.\n",
    "    trainer.test(model, dataloaders=test_loader)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75fb3c69-af00-4219-9e95-af1d74834f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Define Sweep Configuration as a Dictionary (no external YAML)\n",
    "# -------------------------------\n",
    "sweep_config = {\n",
    "    \"program\": \"New_file.py\",  # Ensure this matches your script filename.\n",
    "    \"method\": \"bayes\",      # Options: \"grid\", \"random\", \"bayes\"\n",
    "    \"metric\": {\n",
    "        \"name\": \"val_loss\",\n",
    "        \"goal\": \"minimize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"filter_organization\": {\"values\": [\"constant\", \"doubling\", \"halving\"]},\n",
    "        \"constant_filter\": {\"values\": [32, 64]},\n",
    "        \"base_filter\": {\"values\": [64,128,]},\n",
    "        \"kernel_size\": {\"values\": [3, 5, 7]},\n",
    "        \"conv_activation\": {\"values\": [\"relu\", \"gelu\", \"silu\", \"mish\"]},\n",
    "        \"dense_activation\": {\"values\": [\"relu\", \"gelu\", \"silu\", \"mish\"]},\n",
    "        \"data_augmentation\": {\"values\": [True, False]},\n",
    "        \"batch_norm\": {\"values\": [True, False]},\n",
    "        \"dropout_rate\": {\"values\": [0,0.25]},\n",
    "        \"learning_rate\": {\"values\": [0.0001, 0.001]},\n",
    "        \"dense_neurons\": {\"values\": [256,512]},\n",
    "        \"batch_size\": {\"values\": [32,64]},\n",
    "        \"optimizer\": {\"values\": [\"adam\"]},\n",
    "        \"max_epochs\": {\"value\": 20 },\n",
    "        \"weight_decay\": {\"values\": [0.0, 1e-4,1e-3]},\n",
    "        \"train_data_path\": {\"value\": \"/home/user/kartikey_phd/DA6401/nature_12K/inaturalist_12K/train\"},  # Update with actual path.\n",
    "        \"test_data_path\": {\"value\": \"/home/user/kartikey_phd/DA6401/nature_12K/inaturalist_12K/val\"},    # Update with actual path.\n",
    "        \"project_name\": {\"value\": \"iNaturalist_Sweep_final_2\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Run the Sweep and Launch the Agent \n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the sweep programmatically.\n",
    "    sweep_id = wandb.sweep(sweep_config, project=sweep_config[\"parameters\"][\"project_name\"][\"value\"])\n",
    "    # Launch the wandb agent to run the training function over multiple trials.\n",
    "    wandb.agent(sweep_id, function=run_training)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
